---
title: "MLassignment3"
author: "Suhail AK"
date: "July 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



XG boost

```{r}

library(data.table)
library(caret)
library(xgboost)
library(verification)
#install.packages("MLmetrics")
library(MLmetrics)
library(data.table)
library(ROCR)
library(pROC)



test <- fread("C:/Users/Administrator/Documents/machinelearning/porto-seguros-safe-driver-prediction-dataset/test.csv")

train <- fread("C:/Users/Administrator/Documents/machinelearning/porto-seguros-safe-driver-prediction-dataset/train.csv")

print("Training data size in RAM:");
print(object.size(train), units = 'Mb')

str(train)

cat_vars <- names(train)[grepl('_cat$', names(train))]
cat_vars

train[, (cat_vars) := lapply(.SD, factor), .SDcols = cat_vars]
test[, (cat_vars) := lapply(.SD, factor), .SDcols = cat_vars]



# one hot encode the factor levels
train <- as.data.frame(model.matrix(~. - 1, data = train))
test <- as.data.frame(model.matrix(~ . - 1, data = test))


train_index <- sample(c(TRUE, FALSE), size = nrow(train), replace = TRUE, prob = c(0.8, 0.2))



x_train <- train[train_index, 3:ncol(train)]
y_train <- as.factor(train$target[train_index])

x_test <- train[!train_index, 3:ncol(train)]
y_test <- as.factor(train$target[!train_index])

levels(y_train) <- c("No", "Yes")
levels(y_test) <- c("No", "Yes")

normalizedGini <- function(aa, pp) {
    Gini <- function(a, p) {
        if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
        temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
        temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
        population.delta <- 1 / length(a)
        total.losses <- sum(a)
        null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
        accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
        gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
        sum(gini.sum) / length(a)
    }
    Gini(aa,pp) / Gini(aa,aa)
}


giniSummary <- function (data, lev = "Yes", model = NULL) {
    levels(data$obs) <- c('0', '1')
    out <- normalizedGini(as.numeric(levels(data$obs))[data$obs], data[, lev[2]])  
    names(out) <- "NormalizedGini"
    out
}


trControl = trainControl(
    method = 'cv',
    number = 2,
    summaryFunction = giniSummary,
    classProbs = TRUE,
    verboseIter = TRUE,
    allowParallel = TRUE)

# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow. 
tuneGridXGB <- expand.grid(
    nrounds=c(350),
    max_depth = c(4, 6),
    eta = c(0.05, 0.1),
    gamma = c(0.01),
    colsample_bytree = c(0.75),
    subsample = c(0.50),
    min_child_weight = c(0))


# train the xgboost learner
xgbmod <- train(
    x = x_train,
    y = y_train,
    method = 'xgbTree',
    metric = 'NormalizedGini',
    trControl = trControl,
    tuneGrid = tuneGridXGB)


```




```{r}


# make predictions

#for test data in training set
preds <- predict(xgbmod, newdata = x_test, type = "prob")


#for 0.5 threshold
preds$final <- as.factor(ifelse(preds$Yes>0.5,1,0))
confusionMatrix(preds$final,y_test,positive = "1")



#for 0.3 threshold

preds$final0.3 <- as.factor(ifelse(preds$Yes>0.3,1,0))

confusionMatrix(preds$final0.3,y_test,positive = "1")

#for 0.03 threshold


preds$final0.03 <- as.factor(ifelse(preds$Yes>0.03,1,0))

confusionMatrix(preds$final0.03,y_test,positive = "1")

#for 0.02 threshold

preds$final0.02 <- as.factor(ifelse(preds$Yes>0.02,1,0))
a <- confusionMatrix(preds$final0.02,y_test,positive = "1")




#for kaggle submission final test data

preds_final <- predict(xgbmod, newdata = test, type = "prob")
View(preds_final)

# convert test target values back to numeric for gini and roc.plot functions
levels(y_test) <- c("0", "1")
y_test_raw <- as.numeric(levels(y_test))[y_test]

# Diagnostics
print(xgbmod$results)



# plot the ROC curve
roc.plot(y_test_raw, preds$Yes, plot.thres = c(0.02, 0.03, 0.04, 0.05))


val1 <- a$overall["Accuracy"]

val2 <- a$byClass["F1"]
val3 <- a$overall["Kappa"]

areaa <- roc(y_test_raw,preds$Yes)
area <- auc(areaa)
#accuracy should be low for unbalanced data
df <- data.frame(c(val1,val2,val3,area))
rownames(df)[4] <- "AUC"
library(knitr)
kable(df)

```

```{r}
kable(df)
```
```{r}

# score the predictions against test data
normalizedGini(y_test_raw, preds$Yes)


```

